{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d065b3-cee0-4f59-b897-9c314f76aed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name =  '../../saved_models/Llama3_70B_Instruct/'  # PATH TO LLM \n",
    "\n",
    "# GET original file names for store results\n",
    "folder_path =  'example_test/'   #'CHANGE TO PATH OF THE DATA FOR BIO Test files\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.bio')]\n",
    "\n",
    "# load processed data\n",
    "data = load_dataset('/home/jupyter/20000360102458359xu/LingfeiQian/saved_dataset/YBXL/Bilingual_example_test/') # Path for processed huggingface format test dataset\n",
    "out_dir = \"llm_results\" # path to output files\n",
    "\n",
    "gpu_number = 2 # GPU number\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(max_tokens=512,stop='<EOS>',temperature=0)\n",
    "llm = LLM(model=f\"{model_name}\", tensor_parallel_size = gpu_number, dtype=torch.bfloat16,device = 'auto',max_model_len=2000)  # Create an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72fa7bb-e701-4983-99f3-ffbef4af0a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:10:28.082152Z",
     "iopub.status.busy": "2025-04-23T21:10:28.081896Z",
     "iopub.status.idle": "2025-04-23T21:10:28.087480Z",
     "shell.execute_reply": "2025-04-23T21:10:28.086601Z",
     "shell.execute_reply.started": "2025-04-23T21:10:28.082123Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_list(input_list, batch_size):\n",
    "    batched_list = []\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        batched_list.append(input_list[i:i + batch_size])\n",
    "    return batched_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabec49-f450-4f0b-a2ee-b3d340d59ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# USE LLMS TO INFERENCE\n",
    "prompts_list = batch_list(data['test']['query'], batch_size)\n",
    "\n",
    "outputs = []\n",
    "for i,prompt_list in enumerate(prompts_list):\n",
    "    print (f'batch:{i+1} of total:{len(prompts_list)}', flush=True)\n",
    "    output = llm.generate(prompt_list,sampling_params)\n",
    "    outputs += output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88c79b-629a-4f73-953b-4978f0e4a89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR NER TASK\n",
    "! mkdir out_dir\n",
    "for i, (seq, file_name) in enumerate(zip(outputs,files)):\n",
    "    file_name = file_name.replace('.bio','')\n",
    "    with open(f'{out_dir}/{file_name}.html','w',encoding='utf-8') as f_write:\n",
    "        f_write.write(seq.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b28f4a0-61f7-403d-8f41-b29bfb20db8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:10:57.287180Z",
     "iopub.status.busy": "2025-04-23T21:10:57.286387Z",
     "iopub.status.idle": "2025-04-23T21:10:57.297518Z",
     "shell.execute_reply": "2025-04-23T21:10:57.296449Z",
     "shell.execute_reply.started": "2025-04-23T21:10:57.287122Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <span class=\"Language_Other\">Tibetan</span> interpreter - # '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860dc742-406c-4334-b58f-41b97fdc1a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:13.136338Z",
     "iopub.status.busy": "2025-04-23T21:11:13.135576Z",
     "iopub.status.idle": "2025-04-23T21:11:13.148603Z",
     "shell.execute_reply": "2025-04-23T21:11:13.147473Z",
     "shell.execute_reply.started": "2025-04-23T21:11:13.136290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Your task is to generate an HTML version of an input text, using HTML <span> tags to mark up specific entities.\\n\\n### Entity Markup Guides:\\nUse <span class=\"Language_Fluent\"> to denote a language spoken by the patient fluently.\\nUse <span class=\"Language_Some\"> to denote a language spoken by the patient in moderate level.\\nUse <span class=\"Language_No\"> To denote a language which cannot be spoken or can only be spoken a little by the patient.\\nUse <span class=\"Language_Other\"> to denote a language that not related to patient.\\n\\n### Entity Definitions:\\nLanguage_Fluent: The person speaks the language fluently, including native speakers and those who have achieved nearnative fluency. They can use the language effectively in  various contexts with complete fluency and cultural understanding. Instances where a patient’s fluency is not explicitly stated but can be directly inferred (e.g. mention of interpreter/translator, preference of language on prescription) are included in this definition.\\nLanguage_Some: The person has a moderate level of proficiency in the language. They can understand and use the language for basic communication and simple conversations but are not fully fluent.\\nLanguage_No: The person knows a few words or phrases in the language but cannot use it for basic communication. Their knowledge is very limited and not sufficient for meaningful  interaction. Or the person does not know or speak the language at all. Cannot use this Language to communicate at all.\\nLanguage_Other: Languages mentioned in the text that are not related to the person’s language proficiency. These may be languages discussed in a different context or related to other individuals. \\n\\n\\n### Additional Rules:\\n1. Only annotate the name of the language itself.\\n2. Do not annotate descriptive words about the language proficiency level (e.g. \"some\", \"simple\") or negations (e.g. \"not\", \"no\").\\n3. Hyphenated languages (ex. Chinese-Mandarin) or language connected by \"/\" should be labelled separately for each token.\\n4. Words that could refer to a language but are not used in that context (e.g. Greek yogurt, Irish Catholic, French catheter size) should not be annotated.\\n\\n### Input Text: Tibetan interpreter - # <EOS>\\n### Output Text:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test']['query'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8250e-586f-45b7-9963-ef96fafbecd4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da46e05-e1f2-4173-aa98-30de2311259c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:21.762907Z",
     "iopub.status.busy": "2025-04-23T21:11:21.761664Z",
     "iopub.status.idle": "2025-04-23T21:11:25.570457Z",
     "shell.execute_reply": "2025-04-23T21:11:25.569397Z",
     "shell.execute_reply.started": "2025-04-23T21:11:21.762856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import NavigableString, Tag\n",
    "from glob import glob\n",
    "import spacy\n",
    "import random,os\n",
    "import pandas as pd\n",
    "import time\n",
    "from ner_metrics import classification_report\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "py_nlp = spacy.load (\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2967f5f2-22c7-439d-8bd5-b57f03a47864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:33.932563Z",
     "iopub.status.busy": "2025-04-23T21:11:33.931810Z",
     "iopub.status.idle": "2025-04-23T21:11:33.941002Z",
     "shell.execute_reply": "2025-04-23T21:11:33.939616Z",
     "shell.execute_reply.started": "2025-04-23T21:11:33.932514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_then_concatnate_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('##'):\n",
    "            # If the token starts with '##', merge it with the previous token (remove '##' and concatenate)\n",
    "            merged_tokens[-1] += token[2:]\n",
    "        else:\n",
    "            # Otherwise, add the token as a new element in the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    # Join the tokens with a space to form a sentence\n",
    "    merged_sentence = ' '.join(merged_tokens)\n",
    "    return merged_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4092aa23-0d6e-485e-9b9f-f5b0bab5a401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:34.204261Z",
     "iopub.status.busy": "2025-04-23T21:11:34.203188Z",
     "iopub.status.idle": "2025-04-23T21:11:34.217415Z",
     "shell.execute_reply": "2025-04-23T21:11:34.216116Z",
     "shell.execute_reply.started": "2025-04-23T21:11:34.204215Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bio2html(file):\n",
    "    with open(file,'r') as f_read:\n",
    "        lines = f_read.readlines()\n",
    "    \n",
    "    processed_text = ''\n",
    "    for i, line in enumerate(lines):\n",
    "        token, e_type = line.strip().split('\\t')\n",
    "        if e_type == 'O':\n",
    "            processed_text += token+' '\n",
    "            \n",
    "        if e_type.startswith('B-'):\n",
    "            if i <= len(lines)-2:\n",
    "                if lines[i+1]=='\\n' or lines[i+1].strip().split('\\t')[1]=='O' or lines[i+1].strip().split('\\t')[1].startswith('B-'):\n",
    "                    processed_text += f'<span class=\"{e_type[2:]}\">'+token+'</span> '\n",
    "                else:\n",
    "                    processed_text += f'<span class=\"{e_type[2:]}\">'+token+' '\n",
    "            else:\n",
    "                processed_text += f'<span class=\"{e_type[2:]}\">'+token+'</span> '\n",
    "            \n",
    "        if e_type.startswith('I-'):\n",
    "            if i <= len(lines)-2:\n",
    "                if lines[i+1]=='\\n' or lines[i+1].strip().split('\\t')[1]=='O' or lines[i+1].strip().split('\\t')[1].startswith('B-'):\n",
    "                    processed_text += token+'</span> '\n",
    "                else:\n",
    "                    processed_text += token+' '\n",
    "            else:\n",
    "                processed_text += token+'</span> '\n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89da1a4c-71c4-46f8-8a2c-0dc85557b944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:34.359713Z",
     "iopub.status.busy": "2025-04-23T21:11:34.359058Z",
     "iopub.status.idle": "2025-04-23T21:11:34.370461Z",
     "shell.execute_reply": "2025-04-23T21:11:34.369480Z",
     "shell.execute_reply.started": "2025-04-23T21:11:34.359671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def html2bio(html_path,entity_list):\n",
    "    with open(html_path) as f:\n",
    "        \n",
    "        html = f.read()\n",
    "        \n",
    "        # Parse HTML using BeautifulSoup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Extract text under 'p' tags and convert to BIO format\n",
    "        bio_format = []\n",
    "        \n",
    "\n",
    "        for child in soup.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                child = split_then_concatnate_tokens(child)\n",
    "                for word in child.split():\n",
    "                    bio_format.append(f\"{word}\\tO\\n\")\n",
    "            elif isinstance(child, Tag):\n",
    "                words = split_then_concatnate_tokens(child.get_text()).split()\n",
    "                try:\n",
    "                    entity = child.attrs['class'][0]\n",
    "                except:\n",
    "                    entity = 'O'\n",
    "                if len(words) != 0:\n",
    "                    if entity != 'O' and entity in entity_list:\n",
    "                        bio_format.append(f\"{words[0]}\\tB-{entity}\\n\")\n",
    "                        for word in words[1:]:\n",
    "                            bio_format.append(f\"{word}\\tI-{entity}\\n\")\n",
    "                    else:\n",
    "                        bio_format.append(f\"{words[0]}\\tO\\n\")\n",
    "                        for word in words[1:]:\n",
    "                            bio_format.append(f\"{word}\\tO\\n\")\n",
    "    return bio_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b237a3-0ec2-48a6-a296-13352c3577e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", use_fast=True)\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[/;\\-]|[^\\w\\s]', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adffbcaf-c4c9-4a3e-ad65-680a06f3a98f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:35.887851Z",
     "iopub.status.busy": "2025-04-23T21:11:35.887197Z",
     "iopub.status.idle": "2025-04-23T21:11:35.899645Z",
     "shell.execute_reply": "2025-04-23T21:11:35.898837Z",
     "shell.execute_reply.started": "2025-04-23T21:11:35.887832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ner_metrics import classification_report\n",
    "\n",
    "def get_performance(files, entity_list):\n",
    "    all_pre_tags = []\n",
    "    all_tokens = []\n",
    "    all_gold_tags = []\n",
    "\n",
    "    for file in files:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        gold_html = bio2html(file)\n",
    "        with open('./tmp_html.html', 'w') as f:\n",
    "            f.write(gold_html)\n",
    "        gold_bio = html2bio('./tmp_html.html', entity_list)\n",
    "        tokens = [line.strip().split('\\t')[0] for line in gold_bio]\n",
    "        tags = [line.strip().split('\\t')[-1] for line in gold_bio]\n",
    "\n",
    "        prediction = f'{out_dir}/{file_name}.html'\n",
    "        bio_2 = html2bio(prediction, entity_list)\n",
    "        pre_tokens = [line.strip().split('\\t')[0] for line in bio_2]\n",
    "        pre_tags = [line.strip().split('\\t')[-1] for line in bio_2]\n",
    "        all_tokens += tokens\n",
    "\n",
    "        if len(gold_bio) == len(bio_2):\n",
    "            for token, gold_tag, pre_tag in zip(tokens, tags, pre_tags):\n",
    "                all_gold_tags.append(gold_tag)\n",
    "                all_pre_tags.append(pre_tag)\n",
    "        else:\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token != '':\n",
    "                    match = False\n",
    "                    for i2 in range(i, -1, -1):\n",
    "                        try:\n",
    "                            token_2, tag_2 = bio_2[i2].strip().split('\\t')\n",
    "                        except:\n",
    "                            token_2, tag_2 = None, None\n",
    "                        if token_2 is not None:\n",
    "                            if token in token_2 or token_2 in token:\n",
    "                                match = True\n",
    "                                break\n",
    "                    if not match:\n",
    "                        tag_2 = 'O'\n",
    "                else:\n",
    "                    tag_2 = ''\n",
    "                all_gold_tags.append(tags[i])\n",
    "                all_pre_tags.append(tag_2)\n",
    "\n",
    "    # Get classification reports\n",
    "    lenient = classification_report(tags_true=all_gold_tags, tags_pred=all_pre_tags, mode=\"lenient\")\n",
    "    strict = classification_report(tags_true=all_gold_tags, tags_pred=all_pre_tags, mode=\"strict\")\n",
    "   # print(lenient)\n",
    "    data = []\n",
    "\n",
    "    for entity in strict.keys():\n",
    "        if entity == 'macro avg' or entity == 'micro avg':\n",
    "            continue  # skip these, we will handle overall separately\n",
    "        strict_scores = strict[entity]\n",
    "        lenient_scores = lenient.get(entity, {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0})\n",
    "        data.append({\n",
    "            'entity': entity,\n",
    "            'strict_precision': f\"{float(strict_scores['precision']):.4f}\",\n",
    "            'strict_recall': f\"{float(strict_scores['recall']):.4f}\",\n",
    "            'strict_f1-score': f\"{float(strict_scores['f1-score']):.4f}\",\n",
    "            'lenient_precision': f\"{float(lenient_scores['precision']):.4f}\",\n",
    "            'lenient_recall': f\"{float(lenient_scores['recall']):.4f}\",\n",
    "            'lenient_f1-score': f\"{float(lenient_scores['f1-score']):.4f}\",\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display\n",
    "    print(df.to_string(index=False, justify='left', line_width=1000))\n",
    "    print()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193dce6-104f-4e9c-92f9-9e5718a7ac19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#70b instruct\n",
    "entity_list = ['Language_Fluent','Language_Some','Language_Other','Language_No']\n",
    "\n",
    "folder_path =  'example_test'   #'CHANGE TO PATH OF THE DATA FOR NAMED ENTITY RECOGINITION, THE FORMAT IS BIO FILES IN THIS CASE'\n",
    "\n",
    "files = [folder_path+'/'+f for f in os.listdir(folder_path) if f.endswith('.bio')] # files for NER\n",
    "\n",
    "df = get_performance(files,entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c01aa-ab45-49ef-a9ba-e7cb05e1ec51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
