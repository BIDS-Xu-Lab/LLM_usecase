{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de86e2ae-e659-4698-a375-2f98d594eae4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read and process json files into bio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18eb09-1a53-4039-9234-74382c5e7667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, RegexpTokenizer\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "import torch\n",
    "\n",
    "file_path_list = list_files_in_directory('train_json/All_2') # path of json files.\n",
    "bio_path='example' # path to store the bio files.\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[/;\\-]|[^\\w\\s]', flags=re.UNICODE)\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    try:\n",
    "        items = os.listdir(directory_path)\n",
    "        files = [directory_path+'/'+item for item in items if os.path.isfile(os.path.join(directory_path, item))]\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def tokenize_with_positions(sentence):\n",
    "    tokens = []\n",
    "    positions = []\n",
    "    offset = 0  \n",
    "    for word in tokenizer.tokenize(sentence):\n",
    "        word_start = sentence.index(word, offset)\n",
    "        word_end = word_start + len(word)\n",
    "        tokens.append(word)\n",
    "        positions.append((word_start, word_end))\n",
    "        offset = word_end\n",
    "    return tokens, positions\n",
    "\n",
    "\n",
    "for file in file_path_list:\n",
    "    with open(file, 'r', encoding='utf-8',errors='ignore') as f1, open(bio_path+'/'+file.split('/')[-1].replace('.json','.bio'), 'w') as f2:\n",
    "        try:\n",
    "            data = json.load(f1)\n",
    "        except Exception:\n",
    "            file_error = 1\n",
    "            print(f1)\n",
    "        out_text = ''\n",
    "        content = data[\"content\"]\n",
    "        indexes = data[\"indexes\"]\n",
    "        entity_list = []\n",
    "        file_error = 0\n",
    "        # step 1: Load entities and tags\n",
    "        for i in indexes:\n",
    "            try:\n",
    "                begin = indexes[i]['Token'][0]['begin']\n",
    "                end = indexes[i]['Token'][0]['end']\n",
    "                word = content[begin:end]\n",
    "            except Exception:\n",
    "                (indexes[i])\n",
    "            if ('Entity' in indexes[i]):\n",
    "                try:\n",
    "                    end_entity = indexes[i]['Entity'][0]['end']\n",
    "                    begin_entity = indexes[i]['Entity'][0]['begin']\n",
    "                    tag = indexes[i]['Entity'][0]['semantic']\n",
    "                    entity_list.append({'start_pos':begin_entity, 'end_pos':end_entity, 'entity_type':tag, 'entity': content[begin_entity:end_entity]})\n",
    "                except Exception:\n",
    "                    file_error = 1\n",
    "                    tag = 'O'\n",
    "                    \n",
    "        if file_error==1:\n",
    "            continue\n",
    "        else:\n",
    "            tokens, positions = tokenize_with_positions(content)\n",
    "            \n",
    "            # step 2：Initialize bio tags\n",
    "            bio_tags = ['O'] * len(tokens)\n",
    "            \n",
    "            # step 3：Add tags for tokens based on the position of strings\n",
    "            for entity in entity_list:\n",
    "                ent_start = entity['start_pos']\n",
    "                ent_end = entity['end_pos']\n",
    "                ent_type = entity['entity_type']\n",
    "                for i, (tok_start, tok_end) in enumerate(positions):\n",
    "                    if tok_end <= ent_start or tok_start >= ent_end:\n",
    "                        continue\n",
    "                    if tok_start == ent_start:\n",
    "                        bio_tags[i] = 'B-' + ent_type\n",
    "                    else:\n",
    "                        bio_tags[i] = 'I-' + ent_type\n",
    "        \n",
    "            for token, tag in zip(tokens, bio_tags):\n",
    "                f2.write(f\"{token}\\t{tag}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1c5f2-974c-4ea5-ade5-f5b11e3ebb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training/testing\n",
    "\n",
    "def move_random_files(src_folder, dst_folder, percentage=0.2):\n",
    "    # Ensure src_folder exists\n",
    "    if not os.path.exists(src_folder):\n",
    "        print(f\"Source folder '{src_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Clear or create destination folder\n",
    "    if os.path.exists(dst_folder):\n",
    "        shutil.rmtree(dst_folder)  # Delete existing folder and all contents\n",
    "    os.makedirs(dst_folder)\n",
    "\n",
    "    # Get all files in source folder (ignore subdirectories)\n",
    "    all_files = [f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))]\n",
    "\n",
    "    # Calculate number to move\n",
    "    selected_count = max(1, int(len(all_files) * percentage))\n",
    "    selected_files = random.sample(all_files, selected_count)\n",
    "\n",
    "    # Move files\n",
    "    for file in selected_files:\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        dst_path = os.path.join(dst_folder, file)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        #print(f\"Moved: {file}\")\n",
    "\n",
    "    print(f\"\\nTotal moved: {len(selected_files)} files to '{dst_folder}'.\")\n",
    "\n",
    "# Usage\n",
    "move_random_files(\"example\", \"example_test\", percentage=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeeaa5a-b641-4d11-aeaf-a6e41e532c1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Process bio files for LLM inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbad1af3-5620-4214-96bd-244478f24a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:06:09.181708Z",
     "iopub.status.busy": "2025-04-23T01:06:09.180909Z",
     "iopub.status.idle": "2025-04-23T01:06:11.232480Z",
     "shell.execute_reply": "2025-04-23T01:06:11.229306Z",
     "shell.execute_reply.started": "2025-04-23T01:06:09.181636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path =  'example'   #'CHANGE TO PATH OF THE DATA FOR NAMED ENTITY RECOGINITION, THE FORMAT IS BIO FILES IN THIS CASE'\n",
    "train_files = ['/home/jupyter/20000360102458359xu/LingfeiQian/bilingual_bert/Bilingual_llm/'+folder_path+'/' + f for f in os.listdir(folder_path) if f.endswith('.bio')] # files for NER\n",
    "\n",
    "\n",
    "test_folder_path =  'example_test'   #'CHANGE TO PATH OF THE DATA FOR NAMED ENTITY RECOGINITION, THE FORMAT IS BIO FILES IN THIS CASE'\n",
    "test_files = ['/home/jupyter/20000360102458359xu/LingfeiQian/bilingual_bert/Bilingual_llm/'+test_folder_path+'/' + f for f in os.listdir(test_folder_path) if f.endswith('.bio')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5d067b-86b1-44e6-ac39-ca0380e7c668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:06:52.891472Z",
     "iopub.status.busy": "2025-04-23T01:06:52.890670Z",
     "iopub.status.idle": "2025-04-23T01:06:52.903860Z",
     "shell.execute_reply": "2025-04-23T01:06:52.900525Z",
     "shell.execute_reply.started": "2025-04-23T01:06:52.891410Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_list = ['Language_Fluent', 'Language_Some', 'Language_No', 'Language_Other'] # load tag types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ca8e60-48c8-4086-889b-9a715d5f1dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:29:03.415531Z",
     "iopub.status.busy": "2025-04-23T01:29:03.414739Z",
     "iopub.status.idle": "2025-04-23T01:29:03.430476Z",
     "shell.execute_reply": "2025-04-23T01:29:03.426875Z",
     "shell.execute_reply.started": "2025-04-23T01:29:03.415471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''### Your task is to generate an HTML version of an input text, using HTML <span> tags to mark up specific entities.\n",
    "\n",
    "### Entity Markup Guides:\n",
    "Use <span class=\"Language_Fluent\"> to denote a language spoken by the patient fluently.\n",
    "Use <span class=\"Language_Some\"> to denote a language spoken by the patient in moderate level.\n",
    "Use <span class=\"Language_No\"> To denote a language which cannot be spoken or can only be spoken a little by the patient.\n",
    "Use <span class=\"Language_Other\"> to denote a language that not related to patient.\n",
    "\n",
    "### Entity Definitions:\n",
    "Language_Fluent: The person speaks the language fluently, including native speakers and those who have achieved nearnative fluency. They can use the language effectively in  various contexts with complete fluency and cultural understanding. Instances where a patient’s fluency is not explicitly stated but can be directly inferred (e.g. mention of interpreter/translator, preference of language on prescription) are included in this definition.\n",
    "Language_Some: The person has a moderate level of proficiency in the language. They can understand and use the language for basic communication and simple conversations but are not fully fluent.\n",
    "Language_No: The person knows a few words or phrases in the language but cannot use it for basic communication. Their knowledge is very limited and not sufficient for meaningful  interaction. Or the person does not know or speak the language at all. Cannot use this Language to communicate at all.\n",
    "Language_Other: Languages mentioned in the text that are not related to the person’s language proficiency. These may be languages discussed in a different context or related to other individuals. \n",
    "\n",
    "### Additional Rules:\n",
    "1. Only annotate the name of the language itself.\n",
    "2. Do not annotate descriptive words about the language proficiency level (e.g. \"some\", \"simple\") or negations (e.g. \"not\", \"no\").\n",
    "3. Hyphenated languages (ex. Chinese-Mandarin) or language connected by \"/\" should be labelled separately for each token.\n",
    "4. Words that could refer to a language but are not used in that context (e.g. Greek yogurt, Irish Catholic, French catheter size) should not be annotated.\n",
    "\n",
    "### Input Text: {} <EOS>\n",
    "### Output Text:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27eac72-a192-4e4e-8e7c-0cb6ad63b6f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:11:45.386216Z",
     "iopub.status.busy": "2025-04-23T01:11:45.385462Z",
     "iopub.status.idle": "2025-04-23T01:11:45.411787Z",
     "shell.execute_reply": "2025-04-23T01:11:45.408518Z",
     "shell.execute_reply.started": "2025-04-23T01:11:45.386157Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_punctuation_spacing(text):\n",
    "    # Remove extra spaces before punctuation (e.g., \" hello,world\" → \"hello,world\")\n",
    "    text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
    "    # Ensure there's a space after punctuation (e.g., \"hello,world\" → \"hello, world\")\n",
    "    text = re.sub(r'([,.!?;:])(?=\\S)', r'\\1 ', text)\n",
    "    # Remove extra spaces (e.g., multiple spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def bio_file_to_text_and_highlight(bio_filename):\n",
    "    \"\"\"\n",
    "    Read content from a BIO file and return:\n",
    "      1. Plain text string (without tags) # for LLM input\n",
    "      2. Highlighted text string (entity parts wrapped in <span class=\"TYPE\">...</span>) # for LLM target output\n",
    "\n",
    "    Example：\n",
    "    Input: english B-Language_Some\n",
    "    Output: english and <span class=\"Language_Some\">english</span>\n",
    "    \"\"\"\n",
    "    plain_text = \"\"\n",
    "    highlighted_text = \"\"\n",
    "    current_entity = \"\"\n",
    "    current_type = \"\"\n",
    "\n",
    "    with open(bio_filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if current_entity:\n",
    "                highlighted_text += f'<span class=\"{current_type}\">{current_entity.strip()}</span> '\n",
    "                plain_text += current_entity\n",
    "                current_entity = \"\"\n",
    "                current_type = \"\"\n",
    "            highlighted_text += \"\\n\"\n",
    "            plain_text += \"\\n\"\n",
    "            continue\n",
    "\n",
    "        if \"\\t\" not in line:\n",
    "            continue  # ignore wrong lines\n",
    "\n",
    "        word, tag = line.split(\"\\t\", 1)\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                highlighted_text += f'<span class=\"{current_type}\">{current_entity.strip()}</span> '\n",
    "                plain_text += current_entity\n",
    "            current_type = tag[2:]\n",
    "            current_entity = word + \" \"\n",
    "\n",
    "        elif tag.startswith(\"I-\") and tag[2:] == current_type:\n",
    "            current_entity += word + \" \"\n",
    "\n",
    "        else:\n",
    "            if current_entity:\n",
    "                highlighted_text += f'<span class=\"{current_type}\">{current_entity.strip()}</span> '\n",
    "                plain_text += current_entity\n",
    "                current_entity = \"\"\n",
    "                current_type = \"\"\n",
    "            highlighted_text += word + \" \"\n",
    "            plain_text += word + \" \"\n",
    "\n",
    "    if current_entity:\n",
    "        highlighted_text += f'<span class=\"{current_type}\">{current_entity.strip()}</span> '\n",
    "        plain_text += current_entity\n",
    "    \n",
    "    return (plain_text.strip()), (highlighted_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8272c83-cc17-4e7c-af99-6e9e84c46ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:11:47.023207Z",
     "iopub.status.busy": "2025-04-23T01:11:47.022475Z",
     "iopub.status.idle": "2025-04-23T01:11:47.038949Z",
     "shell.execute_reply": "2025-04-23T01:11:47.036216Z",
     "shell.execute_reply.started": "2025-04-23T01:11:47.023151Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the processed data \n",
    "a,b = bio_file_to_text_and_highlight(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24f814c-2894-4c70-b9e2-bdf0948f8bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:27:23.801517Z",
     "iopub.status.busy": "2025-04-23T01:27:23.800714Z",
     "iopub.status.idle": "2025-04-23T01:27:26.424336Z",
     "shell.execute_reply": "2025-04-23T01:27:26.421033Z",
     "shell.execute_reply.started": "2025-04-23T01:27:23.801457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_prompts = []\n",
    "train_answers = []\n",
    "for file in train_files:\n",
    "    plain, html = bio_file_to_text_and_highlight(file)\n",
    "    train_prompts.append(prompt.format(plain))\n",
    "    train_answers.append(html)\n",
    "\n",
    "\n",
    "test_prompts = []\n",
    "test_answers = []\n",
    "for file in test_files:\n",
    "    plain, html = bio_file_to_text_and_highlight(file)\n",
    "    test_prompts.append(prompt.format(plain))\n",
    "    test_answers.append(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578488c1-b028-4d3e-bb66-d13c561508d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Process data for fine-tuning formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfdb6602-32d3-4d7d-b8a5-e3b03dcdabb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:52:38.425057Z",
     "iopub.status.busy": "2025-04-23T01:52:38.424144Z",
     "iopub.status.idle": "2025-04-23T01:52:38.431628Z",
     "shell.execute_reply": "2025-04-23T01:52:38.430206Z",
     "shell.execute_reply.started": "2025-04-23T01:52:38.424999Z"
    }
   },
   "outputs": [],
   "source": [
    "path_for_training_data = '/home/jupyter/20000360102458359xu/LingfeiQian/saved_dataset/YBXL' # change to path for your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efbd3949-0012-4190-901d-d9b53c6206d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:44:15.127511Z",
     "iopub.status.busy": "2025-04-23T01:44:15.126319Z",
     "iopub.status.idle": "2025-04-23T01:44:15.134180Z",
     "shell.execute_reply": "2025-04-23T01:44:15.132947Z",
     "shell.execute_reply.started": "2025-04-23T01:44:15.127451Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set_input = train_prompts\n",
    "test_set_input = test_prompts\n",
    "\n",
    "train_set_output = train_answers\n",
    "test_set_output = test_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11c1e768-e75f-466d-9c12-22ee25186f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:47:07.042007Z",
     "iopub.status.busy": "2025-04-23T01:47:07.041152Z",
     "iopub.status.idle": "2025-04-23T01:47:07.058739Z",
     "shell.execute_reply": "2025-04-23T01:47:07.057600Z",
     "shell.execute_reply.started": "2025-04-23T01:47:07.041949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "def transfer_local_data_into_huggingface_data(inputs, outputs, task, train_test, training_path = path_for_training_data):\n",
    "    dataset_folder = os.path.join(training_path, f'{task}_{train_test}')\n",
    "    data_folder = os.path.join(dataset_folder, 'data')\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    if train_test == 'train' or train_test == 'val':\n",
    "        data_list = []\n",
    "        for inp, oup in zip(inputs, outputs):\n",
    "            data_list.append([{'role': 'user', 'content': inp}, {'role': 'assistant', 'content': oup}])\n",
    "            \n",
    "        data_df = pd.DataFrame({\"conversations\": data_list})\n",
    "    elif train_test == 'test':\n",
    "        \n",
    "        data_df = pd.DataFrame({\"query\": inputs, \"answer\": outputs})\n",
    "\n",
    "    data_dataset = Dataset.from_pandas(data_df)\n",
    "    dataset_dict = DatasetDict({train_test: data_dataset})\n",
    "    data_path = os.path.join(data_folder, f\"{train_test}.parquet\")\n",
    "    dataset_dict[train_test].to_parquet(data_path)\n",
    "\n",
    "    gitattributes_content = \"\"\"*.parquet filter=lfs diff=lfs merge=lfs -text\"\"\"\n",
    "    with open(os.path.join(dataset_folder, \"gitattributes\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gitattributes_content)\n",
    "    readme_content = \"\"\"# My Hugging Face Conversational Dataset\n",
    "\n",
    "This dataset contains a structured dataset in Hugging Face format with `train` and `test` splits.\n",
    "\n",
    "## Structure\n",
    "- `data/train.parquet`: Training conversations.\n",
    "- `data/test.parquet`: Testing conversations.\n",
    "\n",
    "## Usage\n",
    "To load this dataset in Python:\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"parquet\", data_files={\"train\": \"data/train.parquet\", \"test\": \"data/test.parquet\"})\n",
    "\"\"\" \n",
    "    with open(os.path.join(dataset_folder, \"README.md\"), \"w\", encoding=\"utf-8\") as f: \n",
    "        f.write(readme_content)\n",
    "    return dataset_dict\n",
    "\n",
    "#train_dataset = Dataset.from_pandas(train_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec2c53b0-843e-48a7-ad19-3a0d86957071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:47:07.561603Z",
     "iopub.status.busy": "2025-04-23T01:47:07.560736Z",
     "iopub.status.idle": "2025-04-23T01:47:07.623998Z",
     "shell.execute_reply": "2025-04-23T01:47:07.622932Z",
     "shell.execute_reply.started": "2025-04-23T01:47:07.561546Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 239.99ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 694.88ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_dict = transfer_local_data_into_huggingface_data(train_set_input, train_set_output, 'Bilingual_example', train_test='train')\n",
    "test_dict = transfer_local_data_into_huggingface_data(test_set_input, test_set_output, 'Bilingual_example', train_test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095ee64-9067-4a64-be5b-73b737e00372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36fdc1-6f76-433c-9ed9-f653b6693633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e78b3d-576a-400a-bec5-c28fe6f24d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
